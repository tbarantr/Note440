---
layout: default
title: 4. Replication (& strong consistency model protocols)
parent: Distributed systems for fun and profit
nav_order: 4
---
# Chapter 4: Replication (& strong consistency model protocols)
* The replication problem is one of most important problems in distributed systems
* Replication is a group communication problem
  - What arrangement and communication pattern gives us the performance and availability characteristics we desire?
  - How can we ensure fault tolerance, durability and non-divergence in the face of network partitions and simultaneous node failure?

* Communication pattern stages
  1. (Request) The client sends a request to a server
  1. (Sync) The synchronous portion of the replication takes place
  1. (Response) A response is returned to the client
  1. (Async) The asynchronous portion of the replication takes place

## Synchronous vs Asynchronous replication
* synchronous, or active, or eager, or push, or pessimistic replication
  - N-of-N approach: before a response is returned, it has to be seen and acknowledged by every server in the system
  - the system will be as fast as the slowest server in it
  = the system cannot tolerate the loss of any servers

* asynchronous, or passive, or pull, or lazy replication
  - 1-of-N approach: a response is returned immediately and update propagation occurs sometime later
  - the system is fast
  - the system is also more tolerant of network latency, and can remain available as long as at least one node is up
  - cannot ensure that all nodes in the system always contain the same state

## An overview of major replication approaches
* One way to categorize replication approaches
  - methods that prevent divergence (single copy systems)
  - methods that risk divergence (multi-master systems)

## single-copy consistency systems
* examples
  - 1n messages (asynchronous primary/backup)
  - 2n messages (synchronous primary/backup)
  - 4n messages (2-phase commit, Multi-Paxos)
  - 6n messages (3-phase commit, Paxos with repeated leader election)

## Primary/backup replication (1PC)
* All updates are performed on the primary, and a log of operations (or alternatively, changes) is shipped across the network to the backup replicas
* two variants:
  - asynchronous primary/backup replication (examples: MySQL, MongoDB)
  - synchronous primary/backup replication
* both asynchronous and synchronous replication can only offer weak guarantees, considering
  - asynchronous: failure in the primary before replication happens
  - synchronous: failure in the primary after backup persists and ACKs, but before the primary ACKs the client
* both are also susceptible to split-brain (e.g. both primary and backups are active)
* manual cleanup may be needed to reconcile
* different primary/backup algorithms differ in their handling of failover, offline replicas and so on.

## Two phase commit (2PC)
* Procedure
  - phase 1: voting
    - the coordinator sends the update to all the participants
    - participants can vote to commit or abort; if voting commit, a participant store the update in a write-ahead-log
  - phase 2: decision
    - the coordinator decides the outcome and informs every participant 
    - If all participants voted to commit, then the update is taken from the temporary area and made permanent
* Prevent diverging replicas by rolling back
* prone to blocking, latency-sensitive
* assumes stable data storage
* CA- in the CAP theorem: it is not partition tolerant. The failure model that 2PC addresses does not include network partitions

## Partition tolerant consensus algorithms
* No discussion for Byzantine faults; rarely used in commercial systems
* to enforce single-copy consistency during network partition
  - tricky: not possible to distinguish between a failed remote node and the node being unreachable
  - break sysmmetry between the two separte partitions: allow only one to be active
* Majority decisions
  - use odd number of nodes
  - requiring a majority of nodes (rather than all nodes as in 2PC) to agree on updates
  - the majority parition continue to operate; the minority parition will stop preocessing operations
  - as long as (N/2+1)-of-N nodes are up and accessible, the system can operate
* Roles
  - nodes can have the same roles or distinct roles
  - generally distinct roles (both Paxos and Raft)
    - a single leader/master server; non-leader needs to forward their requests to leader
    - can reassigne roles after a failure via a leader election phase
* Epochs
  - a period of election and normal operations
  - the same leader coordinates until the end of the epoch
  - epochs act as a logical clock--nodes that were out of operations will have a smaller epoch number
* Leader changes
  - all nodes start as followers, and one leader is chosen at the start
  - leader maintains a heartbeat, allows the followers to detect failure/partition
  - followers will switch to an intermediate state when leader is non-responsive
    - increments the term/epoch by one
    - initiates a leader election
    - compete to be the new leader
    - add random amount of waiting time between attempting 
* Numbered proposals within a epoch
  - updates proposed/broadcasted are numbered within each epoch
* Normal operation
  TBH, this part is not very well-exaplained in the book. Should just read [Paxos Made Simple](http://lamport.azurewebsites.net/pubs/paxos-simple.pdf).
```
  [ Proposer ] -> Prepare(n)                                [ Followers ]
             <- Promise(n; previous proposal number
                and previous value if accepted a
                proposal in the past)

  [ Proposer ] -> AcceptRequest(n, own value or the value   [ Followers ]
                associated with the highest proposal number
                reported by the followers)
                <- Accepted(n, value)
```

## Partition-tolerant consensus algorithms: Paxos, Raft, ZAB
Just some fun facts.
* Paxos
  - used in many of Google's systems, including the Chubby lock manager used by BigTable/Megastore, the Google File System as well as Spanner.
  - named after the Greek island of Paxos, and was originally presented by Leslie Lamport in a paper called "The Part-Time Parliament" in 1998. 
  - often considered to be difficult to implement
  - Paxos is described in terms of a single round of consensus decision making, but an actual working implementation usually wants to run multiple rounds of consensus efficiently. 

* ZAB
  - Zookeeper Atomic Broadcast protocol is used in Apache Zookeeper.
  - a system which provides coordination primitives for distributed systems, and is used by many Hadoop-centric distributed systems for coordination (e.g. HBase, Storm, Kafka). 
  - basically the open source community's version of Chubby.

* Raft. 
  - Raft is a recent (2013) addition to this family of algorithms. 
  - easier to teach than Paxos, while providing the same guarantees. 

## Replication methods with strong consistency
Just a summary of the above algorithms.
* Primary/Backup (1PC)
  - Single, static master
  - Replicated log, slaves are not involved in executing operations
  - No bounds on replication delay
  - Not partition tolerant
  - Manual/ad-hoc failover, not fault tolerant, "hot backup"
* 2PC
  - Unanimous vote: commit or abort
  - Static master
  - 2PC cannot survive simultaneous failure of the coordinator and a node during a commit
  - Not partition tolerant, tail latency sensitive
* Paxos 
  - Majority vote
  - Dynamic master
  - Robust to n/2-1 simultaneous failures as part of protocol
  - Less sensitive to tail latency
