---
layout: default
title: 5. Replication - weak consistency model protocols
parent: Distributed systems for fun and profit
nav_order: 5
---
# Chapter 5: Replication: weak consistency model protocols
* enforcing order is expensive
* Eventual consistency
  - Nodes can for some time diverge from each other, but that eventually they will agree on the value
  - Eventual consistency with probabilistic guarantees
    - detect conflicting writes at some later point, but does not guarantee that the results are equivalent to some correct sequential execution
    - Amazon's Dynamo
  - Eventual consistency with strong guarantees
    - guarantees that the results converge to a common value equivalent to some correct sequential execution
* CRDT (convergent replicated data types) are data types that guarantee convergence to the same value in spite of network delays, partitions and message reordering
* The CALM (consistency as logical monotonicity) conjecture equates logical monotonicity with convergence.
  - help to decide when and where to apply coordination techniques

## Reconciling different operation orders
* Scenario 1: each partition handles distinct set of clients
```
[Clients]   - > [A]
--- Partition ---
[Clients]   - > [B]
--- Partition ---
[Clients]   - > [C]
```
```
[A] \
    --> [merge]
[B] /     |
          |
[C] ----[merge]---> result
```

* Scenario 2: without coordination, each host receive updates in different order
```
[Clients]  --> [A]  1, 2, 3
[Clients]  --> [B]  2, 3, 1
```

## Amazon's Dynamo
* eventually consistent, highly available key-value store
* prioritizes availability over consistency
* replicas may diverge from each other when values are written
* there is a read reconciliation phase that attempts to reconcile differences between replicas before returning the value back to the client
* basis for Linkedin's Voldemort, Facebook's Cassandra and Basho's Riak
```
[ Client ]
    |
( Mapping keys to nodes )
    |
    V
[ Node A ]
    |     \
( Synchronous replication task: minimum durability )
    |        \
[ Node B]  [ Node C ]
    A
    |
( Conflict detection; asynchronous replication task:
  ensuring that partitioned / recovered nodes recover )
    |
    V
[ Node D]
```
### Consistent hashing
* key-to-node mapping can be calculated on client

### Partial quorums
* synchronous for higher level of duratibility
* sloppy (partial) quorums instead of strict (majority) quorums
* different subsets of the quorum may contain different versions of data
* the user choose:
  - W-of-N nodes required for a write
  - R-of-N nodes required for a read
  - usually `N = 3`
  - usual recommendation: `R + W > N`:
    - `R = 1, W = N`: fast reads, slow writes
    - `R = N, W = 1`: fast writes, slow reads
    - `R = N/2` and `W = N/2 + 1`: favorable to both
  - how many messages per read/write?
    - `N`: less sensitive to latency, less efficient (more messages to send)
    - `R/W`: more sensitive to latency, more efficient
* this is not "strong consistency"
  - node failures => a different unrelated node will be added; even quorum sizes are equal to N, the nodes in those quorums can change
  -during partition, writes might be allowed on both sides of a partition; for some time the system does not act as a single copy

### Conflict detection and read repair
* systems that allow diverging replicas eventually need to reconcile two different values
* generally, this is done by tracking the history of data by cupplementing it with some metadata, like vector clocks
* alternatives:
  - __no metadata__: nothing can be done for conflicting writes
  - __timestamps__: higher timstamp wins; susceptible to poorly synchronized clock, like faulty/fast clock
  - __version numbers__: can avoid some issues related to timestamps, but not enough to track causality
  - __vector clocks__:
    - can detect concurrent and out of date updates, but sometimes the client needs to picka value
    - takes all responses and discard values taht are strictly older
      - if there is only one {vector clock, value} pair, it returns that
      - otherwise, return all pairs

### Replica synchronization: gossip and Merkle trees
* a way to deal with nodes rejoining the cluster
* synchronization happens after a failure and periodically
* Gossip
  - a probabilistic technique for synchronizing replicas
  - pattern of communication not determined in advance
  - every `t` seconds, each nodes picks a node to communicate
  - scalable, no single point of failure, but only provides probabilistic guarantees
* Merkle trees
  - a hashing mechanism
  - a data store can be hashed at multiple different levels of granularity, representing the whole content, half of the key spaces, a quarter of the keys, etc.
  - nodes can use hashing to compare data store content more efficiently

### Dynamo in practice: probabilistically bounded staleness (PBS)
* PBS uses Monte Carlo simulation to characterize the expected behavior of such a system
* real world informatiion needed:
  - anti-entropy (gossip) rate
  - network latency
  - local processing delay

## Convergent replicated datatypes (CRDTs)
* CRDTs exploit knowledge regarding the commutativity and associativity of specific operations on specific datatypes.
